{
 "metadata": {
  "name": "",
  "signature": "sha256:ce6ded1766e0fc50b3b2f03bb02f015734f84b83b6768ad29c3bf733193e7591"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "\n",
      "from matplotlib.pyplot import imshow\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "import _metrics"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The autoreload extension is already loaded. To reload it, use:\n",
        "  %reload_ext autoreload\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import cv2\n",
      "import glob\n",
      "import numpy as np\n",
      "import mahotas as mh\n",
      "import os\n",
      "import uuid\n",
      "import tifffile as tif\n",
      "from scipy import ndimage as nd\n",
      "from scipy.misc import imrotate\n",
      "import skimage.measure\n",
      "from skimage import img_as_ubyte\n",
      "import random\n",
      "import cPickle as pickle\n",
      "import time\n",
      "import partition_comparison\n",
      "\n",
      "DATA_PATH = '/Volumes/DATA1/EMQM_DATA/ac3x75/'\n",
      "GOLD_PATH = os.path.join(DATA_PATH,'gold/')\n",
      "RHOANA_PATH = os.path.join(DATA_PATH,'rhoana/')\n",
      "IMAGE_PATH = os.path.join(DATA_PATH,'input/')\n",
      "PROB_PATH = os.path.join(DATA_PATH,'prob/')\n",
      "PATCH_PATH = os.path.join(DATA_PATH,'test_rhoana/')\n",
      "\n",
      "\n",
      "gold = _metrics.Util.read(GOLD_PATH+'*.tif')\n",
      "rhoana = _metrics.Util.read(RHOANA_PATH+'*.tif')\n",
      "images = _metrics.Util.read(IMAGE_PATH+'*.tif')\n",
      "probs = _metrics.Util.read(PROB_PATH+'*.tif')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loaded 75 images.\n",
        "Loaded"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 75 images.\n",
        "Loaded"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 75 images.\n",
        "Loaded"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 75 images.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "tifffile.py:1995: UserWarning: failed to import _tifffile.decodepackbits\n",
        "  warnings.warn(\"failed to import %s\" % module_function)\n",
        "tifffile.py:1995: UserWarning: failed to import _tifffile.decodelzw\n",
        "  warnings.warn(\"failed to import %s\" % module_function)\n",
        "tifffile.py:1995: UserWarning: failed to import _tifffile.unpackints\n",
        "  warnings.warn(\"failed to import %s\" % module_function)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 226
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\n",
      "#\n",
      "# let the CNN rate these for split errors\n",
      "#\n",
      "#\n",
      "def test_n(image, path, testfile, targetfile, only_show_bad=False, draw_bbox=True):\n",
      "    from test_cnn_vis import TestCNN\n",
      "    t = TestCNN('7b76867e-c76a-416f-910a-7065e93c616a', 'patches_large2new')\n",
      "    val_fn = t.run()\n",
      "    data, targets = t.load_dataset(path, testfile, targetfile)\n",
      "    for batch in t.iterate_minibatches(data, targets, len(targets), shuffle=False):\n",
      "            images, probs, binary1s, binary2s, overlaps, bboxs, borders, targets = batch\n",
      "            pred, err, acc = val_fn(images, probs, binary1s, binary2s, overlaps, targets)\n",
      "            \n",
      "    patches = []\n",
      "    \n",
      "    for i,p in enumerate(pred):\n",
      "        \n",
      "        bbox = bboxs[i]\n",
      "        border = borders[i]\n",
      "        target = targets[i]\n",
      "        \n",
      "        if isinstance (bbox, (np.ndarray, np.generic)) == False:\n",
      "            if isinstance (bbox, list) == False:\n",
      "                continue\n",
      "\n",
      "        # check if bbox is in our BB\n",
      "        if bbox[0] >= BB[0] and bbox[1] <= BB[1] and bbox[2] >= BB[2] and bbox[3] <= BB[3]:\n",
      "                \n",
      "            patches.append([images[i], probs[i], binary1s[i], binary2s[i], bbox, border, p[1]])\n",
      "        \n",
      "    return patches\n",
      "    \n",
      "\n",
      "\n",
      "def gen_error_view(gold, rhoana, spliterrors, sureness=1., outdir='/Volumes/DATA1/EMQM_DATA/ac3x75/RHOANA_SPLIT_CORRECTION/SMALL/'):\n",
      "\n",
      "    store_split_errors('rhoana', rhoana, spliterrors, sureness, outdir)\n",
      "    store_split_errors('gold', gold, spliterrors, sureness, outdir)\n",
      "    \n",
      "    #return create_merge_table(rhoana, spliterrors, sureness)\n",
      "    \n",
      "def create_merge_table(arr, patches, requested_sureness_for_spliterror):\n",
      "    '''\n",
      "    '''\n",
      "\n",
      "    merge_table = {}\n",
      "    surenesses = {}\n",
      "    \n",
      "    for i,s in enumerate(patches):\n",
      "        \n",
      "        bbox = s[4]\n",
      "        label1 = s[2]\n",
      "        label2 = s[3]\n",
      "        sureness_for_spliterror = s[6]  # 0..1 for spliterror der gemerged werden soll\n",
      "        \n",
      "        \n",
      "        if sureness_for_spliterror < requested_sureness_for_spliterror:\n",
      "           continue\n",
      "        \n",
      "        cropped_seg = np.array(arr[bbox[0]-BB[0]:bbox[1]-BB[1]+1, bbox[2]-BB[2]:bbox[3]-BB[3]+1])\n",
      "        cropped_seg2 = np.array(cropped_seg)\n",
      "        \n",
      "        cropped_seg[label1.reshape(75,75) == 0] = 0\n",
      "        cropped_seg2[label2.reshape(75,75) == 0] = 0\n",
      "\n",
      "        labelvalue1 = cropped_seg.max()\n",
      "        labelvalue2 = cropped_seg2.max()\n",
      "        \n",
      "        if labelvalue1 == 0 or labelvalue2 == 0:\n",
      "            print 'err'\n",
      "            continue\n",
      "\n",
      "        minlabel12 = min(labelvalue1, labelvalue2)\n",
      "        maxlabel12 = max(labelvalue1, labelvalue2)\n",
      "        \n",
      "\n",
      "#         if minlabel12 in merge_table:\n",
      "#             if not maxlabel12 in merge_table[minlabel12]:\n",
      "#                 merge_table[minlabel12].append(maxlabel12)\n",
      "#         else:\n",
      "\n",
      "        already_merged = False\n",
      "        already_merged_where = -1\n",
      "        for k,i in enumerate(merge_table.items()):\n",
      "            if minlabel12 in i or maxlabel12 in i:\n",
      "                already_merged = True\n",
      "                already_merged_where = k\n",
      "                \n",
      "        #print minlabel12, merge_table.items(), surenesses.items(), already_merged, already_merged_where\n",
      "\n",
      "        if already_merged:\n",
      "            # check if the sureness is higher than the one before\n",
      "#             print minlabel12, 'there', merge_table[already_merged_where], surenesses[already_merged_where], sureness_for_spliterror\n",
      "            if surenesses.items()[already_merged_where][1] < sureness_for_spliterror:\n",
      "                # then replace\n",
      "                del merge_table.items()[already_merged_where]\n",
      "                del surenesses.items()[already_merged_where]\n",
      "                merge_table[minlabel12] = maxlabel12#, sureness_for_spliterror]\n",
      "                surenesses[minlabel12] = sureness_for_spliterror\n",
      "        else:\n",
      "            merge_table[minlabel12] = maxlabel12#, sureness_for_spliterror]\n",
      "            surenesses[minlabel12] = sureness_for_spliterror\n",
      "\n",
      "\n",
      "#     for k in merge_table.keys():\n",
      "#         merge_table[k] = sorted(merge_table[k] + [k])\n",
      "            \n",
      "#     merge_table = process_merge_table(merge_table)\n",
      "            \n",
      "        \n",
      "    return merge_table\n",
      "    \n",
      "\n",
      "def process_merge_table(mt_):\n",
      "    import copy\n",
      "    mt = copy.deepcopy(mt_)\n",
      "    for k1 in mt.keys():\n",
      "        for k2 in mt.keys():\n",
      "            if k2 > k1:\n",
      "                if k1 in mt.keys() and k2 in mt.keys():\n",
      "                    anzahl_gleicher_elemente = len(list(set(mt[k1]) & set(mt[k2])))\n",
      "                    if anzahl_gleicher_elemente > 0:\n",
      "                        mt[k1] += mt[k2]\n",
      "                        del mt[k2]\n",
      "        #mt[k1] = sorted(list(set(mt[k1])))\n",
      "    return mt    \n",
      "    \n",
      "    \n",
      "def store_split_errors(name, arr, spliterrors, sureness, outdir):\n",
      "    \n",
      "    out = np.zeros((arr.shape[0], arr.shape[1], 4))\n",
      "\n",
      "    out[:,:,0] = arr[:,:,0]\n",
      "    out[:,:,1] = arr[:,:,1]\n",
      "    out[:,:,2] = arr[:,:,2]\n",
      "    out[:,:,3] = 64\n",
      "\n",
      "    for i,s in enumerate(spliterrors):\n",
      "\n",
      "        ERROR = i\n",
      "\n",
      "        if spliterrors[ERROR][6] >= sureness:\n",
      "            color = (255,0,0,255)\n",
      "        else:\n",
      "            color = (0,255,0,255)\n",
      "\n",
      "        bbox = spliterrors[ERROR][4]\n",
      "\n",
      "        border = spliterrors[ERROR][5]\n",
      "        for c in border:\n",
      "            \n",
      "\n",
      "            out[c[0]-BB[0], c[1]-BB[2]] = color\n",
      "\n",
      "    mh.imsave(os.path.join(outdir, 'compare_'+str(name)+'_'+str(sureness)+'.tif'), out.astype(np.uint8))\n",
      "\n",
      "    \n",
      "def apply_merge_table(arr, mergetable):\n",
      "    \n",
      "    out = np.array(arr)\n",
      "    \n",
      "    for k in mergetable.keys():\n",
      "#         labels = mergetable[k]\n",
      "        \n",
      "#         for l in labels:\n",
      "#             out[out == l] = k\n",
      "        l = mergetable[k]\n",
      "        #print 'merging', l, k\n",
      "        out[out == k] = l\n",
      "    \n",
      "    return out\n",
      "\n",
      "\n",
      "def fix_splits(rhoana_normalized, gold_normalized, patches, slice, sureness=1., outdir='/Volumes/DATA1/EMQM_DATA/ac3x75/RHOANA_SPLIT_CORRECTION/SMALL/'):\n",
      "    '''\n",
      "    '''\n",
      "    \n",
      "    # create folder for this sureness value\n",
      "    sureness_folder = os.path.join(outdir, str(slice), str(sureness))\n",
      "    if not os.path.exists(sureness_folder):\n",
      "        os.makedirs(sureness_folder)\n",
      "    \n",
      "    # colorize rhoana and gold\n",
      "    cm = _metrics.Util.load_colormap('/Volumes/DATA1/ac3x75/mojo/ids/colorMap.hdf5')\n",
      "    colored_rhoana_normalized = cm[rhoana_normalized % len(cm)]\n",
      "    colored_gold_normalized = cm[gold_normalized % len(cm)]\n",
      "    \n",
      "    # create border views\n",
      "    gen_error_view(colored_gold_normalized, colored_rhoana_normalized, patches, sureness=sureness, outdir=sureness_folder)\n",
      "    # create all fix tables\n",
      "    merge_table = create_merge_table(rhoana_normalized, patches, sureness)\n",
      "    # .. and apply them\n",
      "#     print len(_metrics.Util.get_histogram(rhoana_normalized.astype(np.uint64)))\n",
      "    applied_mt = apply_merge_table(rhoana_normalized, merge_table)\n",
      "#     print len(_metrics.Util.get_histogram(applied_mt.astype(np.uint64)))    \n",
      "    \n",
      "    # .. colorize result\n",
      "    colored_applied_mt = cm[applied_mt % len(cm)]\n",
      "    \n",
      "    mh.imsave(os.path.join(sureness_folder, 'rhoana.tif'), colored_applied_mt)\n",
      "    \n",
      "    # print VI\n",
      "    vi = partition_comparison.variation_of_information(applied_mt.ravel(), gold_normalized.ravel())\n",
      "    #print 'Sureness:', sureness, 'VI:', vi\n",
      "    \n",
      "    return vi, applied_mt, merge_table\n",
      "\n",
      "\n",
      "#\n",
      "#\n",
      "# now grab border patches in the rhoana segmentation\n",
      "#\n",
      "#\n",
      "def grab_patches(images, probs, rhoana):\n",
      "    NO_PATCHES = 10000\n",
      "\n",
      "    p_target = np.zeros(NO_PATCHES)        \n",
      "\n",
      "    total = t0 = time.time()\n",
      "    patches = 0\n",
      "    data = []\n",
      "\n",
      "    PATCH_BYTES = 75*75\n",
      "    p_image = np.zeros((NO_PATCHES, PATCH_BYTES),dtype=np.uint8)\n",
      "    p_prob = np.zeros((NO_PATCHES, PATCH_BYTES),dtype=np.uint8)\n",
      "    p_label1 = np.zeros((NO_PATCHES, PATCH_BYTES),dtype=np.uint8)\n",
      "    p_label2 = np.zeros((NO_PATCHES, PATCH_BYTES),dtype=np.uint8)\n",
      "    p_overlap = np.zeros((NO_PATCHES, PATCH_BYTES),dtype=np.uint8)\n",
      "    p_bbox = [0]*NO_PATCHES\n",
      "    p_thumb = np.zeros((NO_PATCHES, PATCH_BYTES*3),dtype=np.uint8)\n",
      "    p_border = [0]*NO_PATCHES\n",
      "    for s in _metrics.MergeError.generate(images, probs, rhoana, 1, thumb=False, rotate=False, flip=False, randomize_slice=False, randomize_label=False, max_per_slice=10000, fill_labels=False):\n",
      "#         print s\n",
      "        p_image[patches] = s._meta['image'].ravel()\n",
      "        p_prob[patches] = s._meta['prob'].ravel()\n",
      "        p_label1[patches] = s._meta['label1'].ravel()\n",
      "        p_label2[patches] = s._meta['label2'].ravel()\n",
      "        p_overlap[patches] = s._meta['overlap'].ravel()\n",
      "        #print s._meta['bbox']\n",
      "        p_bbox[patches] = s._meta['bbox']\n",
      "        #p_thumb[patches] = s._thumb.ravel()\n",
      "        p_border[patches] = s._meta['border']\n",
      "\n",
      "        t1 = time.time()\n",
      "        total = t1-t0\n",
      "\n",
      "        patches += 1\n",
      "        if patches % 1000 == 0:\n",
      "            print 'Another 1000 generated after',total,'seconds'    \n",
      "\n",
      "    #     if total > 100:\n",
      "    #         break\n",
      "\n",
      "        if patches >= NO_PATCHES:\n",
      "            break\n",
      "\n",
      "    print total, 'seconds for', patches, 'patches'\n",
      "\n",
      "#     return p_image, p_prob, p_label1, p_label2, p_overlap, p_bbox, p_border, p_thumb, p_target\n",
      "    return p_image, p_prob, p_label1, p_label2, p_overlap, p_bbox, p_border, p_target\n",
      "\n",
      "# now store the patches\n",
      "def store_patches(filename, patches):\n",
      "    \n",
      "    np.savez(PATCH_PATH+filename+'.npz', image=patches[0], prob=patches[1], binary1=patches[2], binary2=patches[3], overlap=patches[4], bbox=patches[5], border=patches[6])\n",
      "    np.savez(PATCH_PATH+filename+'_targets.npz', targets=patches[-1])\n",
      "    \n",
      "    \n",
      "def propagate_max_overlap(rhoana, gold):\n",
      "\n",
      "    out = np.array(rhoana)\n",
      "    \n",
      "    rhoana_labels = _metrics.Util.get_histogram(rhoana.astype(np.uint64))\n",
      "    \n",
      "    for l,k in enumerate(rhoana_labels):\n",
      "        if l == 0:\n",
      "            # ignore 0 since rhoana does not have it\n",
      "            continue\n",
      "        values = gold[rhoana == l]\n",
      "        largest_label = _metrics.Util.get_largest_label(values.astype(np.uint64))\n",
      "    \n",
      "        out[rhoana == l] = largest_label # set the largest label from gold here\n",
      "    \n",
      "    return out\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#SLICE = 70\n",
      "for SLICE in range(70,73):\n",
      "    \n",
      "    \n",
      "\n",
      "    #BB = [150,300,550,650]\n",
      "    BB = [0,1024,0,1024]\n",
      "    #BB = [170,350,390,550]\n",
      "\n",
      "    rhoana_normalized = _metrics.Util.normalize_labels(rhoana[SLICE])[0]\n",
      "\n",
      "    # fill and normalize gold\n",
      "    gold_zeros = _metrics.Util.threshold(gold[SLICE], 0)\n",
      "    gold_filled = _metrics.Util.fill(gold[SLICE], gold_zeros.astype(np.bool))\n",
      "    gold_filled_relabeled = skimage.measure.label(gold_filled).astype(np.uint64)\n",
      "    gold_normalized = _metrics.Util.normalize_labels(gold_filled_relabeled)[0]\n",
      "\n",
      "    # color both segmentations\n",
      "    cm = _metrics.Util.load_colormap('/Volumes/DATA1/ac3x75/mojo/ids/colorMap.hdf5')\n",
      "    colored_rhoana_normalized = cm[rhoana_normalized % len(cm)]\n",
      "    colored_gold_normalized = cm[gold_normalized % len(cm)]\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    outdir = '/Volumes/DATA1/EMQM_DATA/ac3x75/RHOANA_SPLIT_CORRECTION/SMALL/'\n",
      "    \n",
      "    PATCH_PATH = os.path.join(outdir,str(SLICE))+os.sep\n",
      "    if not os.path.exists(PATCH_PATH):\n",
      "        os.makedirs(PATCH_PATH)\n",
      "    \n",
      "    #\n",
      "    # generate patches for slice\n",
      "    if not os.path.exists(PATCH_PATH +os.sep+ 'all.npz'):\n",
      "        print 'need new patches..'\n",
      "        patches_all = grab_patches(images[SLICE:SLICE+1], probs[SLICE:SLICE+1], rhoana_normalized.astype(np.uint64))\n",
      "        store_patches('all',patches_all)\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    patches = test_n(images[SLICE], PATCH_PATH, 'all.npz', 'all_targets.npz')\n",
      "    vi_log = ''\n",
      "    r = range(0,101)\n",
      "    vi_s = []#[0]*len(r)\n",
      "    before_VI = partition_comparison.variation_of_information(gold_normalized[BB[0]:BB[1],BB[2]:BB[3]].ravel(), rhoana_normalized[BB[0]:BB[1],BB[2]:BB[3]].ravel())\n",
      "    best = np.inf\n",
      "    for s in r:\n",
      "        s /= 100.\n",
      "        #fix_splits(rhoana_normalized, gold_normalized, patches, slice=SLICE, sureness=s)\n",
      "        vi,i,mt = fix_splits(rhoana_normalized[BB[0]:BB[1],BB[2]:BB[3]], gold_normalized[BB[0]:BB[1],BB[2]:BB[3]], patches, slice=SLICE, sureness=s)\n",
      "        if vi < before_VI:\n",
      "            result = 'GOOD'\n",
      "        else:\n",
      "            result = 'BAD'\n",
      "        new_best = min(best, vi)\n",
      "        if new_best != best:\n",
      "            best_sureness = s\n",
      "            best = new_best\n",
      "        vi_log += result + ' Sureness: ' + str(s) + ' VI: ' + str(vi) + '\\n'\n",
      "        vi_s.append(vi)\n",
      "    print 'SLICE', SLICE, 'BEST', best, 'OLD VI:', before_VI, 'SURENESS', best_sureness\n",
      "\n",
      "\n",
      "    # create plot\n",
      "\n",
      "    new_rhoana = propagate_max_overlap(rhoana_normalized, gold_normalized)\n",
      "    target_vi = partition_comparison.variation_of_information(new_rhoana.ravel(), gold_normalized.ravel())\n",
      "    \n",
      "    bins = np.arange(0, len(vi_s))\n",
      "    # bins /= 100\n",
      "    original_vi = [before_VI]*101\n",
      "    target_vi = [target_vi]*101\n",
      "\n",
      "    fig, ax = plt.subplots()\n",
      "\n",
      "    ax.plot(bins, target_vi, 'k--', label='Target VI')\n",
      "    ax.plot(bins, vi_s, 'k', label='Variation of Information')\n",
      "    ax.plot(bins, original_vi, 'k:', label='Rhoana VI before')\n",
      "    # ax.set_yscale('log')\n",
      "\n",
      "    # Now add the legend with some customizations.\n",
      "    legend = ax.legend(loc='upper center', shadow=True)\n",
      "    ax.set_ylim([0.6,1.5])\n",
      "\n",
      "    # The frame is matplotlib.patches.Rectangle instance surrounding the legend.\n",
      "    frame = legend.get_frame()\n",
      "    frame.set_facecolor('0.90')\n",
      "\n",
      "    # Set the fontsize\n",
      "    for label in legend.get_texts():\n",
      "        label.set_fontsize('large')\n",
      "\n",
      "    for label in legend.get_lines():\n",
      "        label.set_linewidth(1.5)  # the legend line width\n",
      "\n",
      "    \n",
      "\n",
      "    # create folder for this sureness value\n",
      "    sureness_folder = os.path.join(outdir, str(SLICE))\n",
      "    if not os.path.exists(sureness_folder):\n",
      "        os.makedirs(sureness_folder)\n",
      "\n",
      "    plt.savefig(os.path.join(sureness_folder,'vi.tif'))\n",
      "\n",
      "    with open(os.path.join(sureness_folder,'out.txt'), 'w') as f:\n",
      "        f.write(vi_log)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "os.path.dirname('/Volumes/DATA1/EMQM_DATA/ac3x75/RHOANA_SPLIT_CORRECTION/SMALL/)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "71"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}