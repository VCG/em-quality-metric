\section{Introduction}

% JT: Trying to write short, as we have only 8 pages.
\JT{EVERYONE: I am a bit concerned about two reductionist arguments that will be thrown at us: 1. why don't we just try and make the initial segmentation better? 2. at the limit, someone still has to scan the whole volume because your edge error classifier is often wrong. Any ideas?}

%Paragraph one: Provide context to the work.
%What is the task? What is the state of the task?
In connectomics, neuroanatomists build 3D reconstructions of neuron connectivity to gain insight into the functional structure of the brain. Thanks to rapid progress in automatic sample preparation and electron microscopy (EM) image acquisition techniques, it is possible to image significant volumes of brain tissue with a resolution high enough to identify synapses and vesicles---at $6nm$ per pixel. For a section thickness of $25nm$, a $1 mm^3$ volume of brain tissue contains $10^{15}$ voxels, or 7 petabytes of data \VKF{please check these numbers}. Manual annotation of data sets of this size is unfeasible, and automatic segmentation methods are needed. State of the art methods use convolutional neural networks to learn cell membranes in 2D images from hand-labeled training data. Then, labeled membranes are grouped into geometrically-consistent cell regions across sections to form 3D objects. Using dynamic programming techniques \VKF{cite cdd whole image training paper}, and small GPU clusters \VKF{how many?}, these classifiers can segment about 1 terabyte of data per hour \VKF{cite pipeline paper, cite seungs newest paper, others?}, which is the rate necessary to keep up with the data capture process on state-of-the-art electron microscopes \VKF{citation for 61 beam microscope?}. 

%Paragraph two: What is the problem in this context? What is the situation that you are trying to correct or overcome?
However, all automatic methods are at least somewhat erroneous, and we are left with large volumes of data which need \emph{proofreading}. This crucial task serves two purposes: 1) to correct errors in the segmentation for later use, and critically 2) to provide larger corpora of labeled data to train better automatic segmentation methods. Recent interactive proofreading tools provide intuitive user interfaces to browse segmentation data in 2D and 3D and identify and manually correct errors \cite{raveler,mojo2,haehn_dojo_2014}. Many kinds of errors exist, such as inaccurate boundaries, but the most egregious are \emph{split errors}, where a single cell is labeled as two, and \emph{merge errors}, where two cells are labeled as one. With user interaction, split errors can be joined, and the missing edge in merge errors can be discovered with techniques like manually-seeded watersheds \VKF{does raveler do this?} (e.g., \cite{haehn_dojo_2014}). \VKF{IMPORTANT: check if raveler or any other proofreading tool does do automatic detection of possible errors and guides the user if not we have a nice contribution here.} However, even with these `semi-automatic' correction tools, the visual inspection of the data to find the errors in the first place takes the majority of the time.

%Paragraph three: What is the proposed solution at a high level? What is the result of the method, and how does it impact the problem?
Our goal is to complement semi-automatic correction tools with semi-automatic error finding. Instead of having to visually inspect the whole data volume carefully to spot any errors, we design two automatic classifiers which detect both split and merge errors in 2D segmentations, to direct the user to regions with a high probability of an error. Then, we suggest probable error corrections for the user to accept or reject. Given an initial membrane segmentation from an automatic method, our classifiers operate on whole cell regions. This significantly reduces the data volume and allows us to employ large convolutional neural networks which take a greater region of context and multiple input channels into account.

%Paragraph four: Justify why this approach is worthwhile. What is the point in trying to learn to classify split or merge edges specifically? Why don't we just try and make the initial segmentation better?
\JT{I don't have a good argument for why we wouldn't just try and make the initial segmentation better.}

%Paragraph five: Optimistically, what is the consequence of the method - what can I do now that I could not do before?
We validate our approach in qualitative studies against existing state-of-the-art approaches, and in comparison to the improvement that is made by humans using an existing proofreading tool. We discover that our method \JT{hopefully: significantly decreases the time taken to reduce errors in segmentation VI}. As a consequence, we are able to produce ground truth labelings more quickly, from which to improve automatic segmentation methods and better tackle vast volumes of connectomics imagery.

\begin{figure}
\missingfigure{Example of a fixed split error. Example of a fixed merge error.}
\caption{Example of a fixed split error. Example of a fixed merge error.}
\end{figure}