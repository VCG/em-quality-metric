\section{Evaluation}
\label{sec:evaluation}
We evaluate our split and merge error detection and correction recommendation in the context of interactive proofreading tools: to direct users to regions with a high probability of error and to suggest corrections (Fig.~\ref{fig:results}). For comparison, we take publicly available mouse cortex data of the same kind as our training data. This data is part of the ISBI 2013 challenge training dataset ($1024\times1024\times100$ voxels) which was acquired using a serial section scanning electron microscope (ssSEM) with a resolution of $6\times6\times30\, nm$ per voxel. We use the available manually-labeled ground truth to score our approach using the variation of information (VI) metric, which is closely related to mutual information. VI is a measure of the distance between two clusterings, where lower VI numbers are better. Since our classifiers are trained on 2D image slices, we perform all evaluations on slices rather than 3D volumes.

\begin{figure}[t]
\centering
	%\subfloat[Segmentation test cases\label{fig:results}]{%
%	\includegraphics[scale=.2]{gfx/results_0129AM_latest.pdf}
\includegraphics[scale=.2]{gfx/all_users_vi_dojo.pdf}
%	\includegraphics[scale=.10]{gfx/new_er.pdf}
	%}
	%\subfloat[Simulated user VI vs. user error rate\label{fig:simusererrorrate}]{%
	%\includegraphics[scale=.08]{gfx/results_0129AM_latest.pdf}
	%}
%\includegraphics[scale=.1]{gfx/user_error_rate.pdf}
\caption{(a) We compare distributions of VI measures across 10 sections for the initial automatic segmentation, the average and best users in the Haehn et al.~experiment with Dojo, a novice and two experts using our system, our simulated user, our simulated user when presented with random recommendations, and finally a fully-automatic correction of recommended errors based on a threshold of acceptance. Lower scores are better.}
\label{fig:results}
\vspace{-0.4cm}
\end{figure}

\paragraph{Interactive proofreading.}
Recently, Haehn et al.~discussed requirements for interactive proofreading and evaluated three different tools on connectomics data in a study with naive users~\cite{haehn_dojo_2014}. This study asked users to spend 30 minutes proofreading with the different tools, to correct split and merge errors to improve the automatic segmentation. The best performing tool in their evaluation was Dojo. We use their findings and their user-generated proofreading result data, which they kindly provided, as a baseline for the evaluation of our method.
%The authors performed a non-expert user study and stated that their software Dojo provides better results than other tools due to a minimalistic user interface and sophisticated 3D volume rendering.
Haehn et al. perform their user study on the most representative sub-volume ($400\times400\times10$ voxels) in terms of distribution of object size. For optimal comparison, we use exactly the same data. We asked a novice and two experts to perform the proofreading task using our system (Fig.~\ref{fig:prototype}).

In addition, we simulate a user for proofreading correction. We assume that all classification has been computed ahead of time, and that the user is presented with a stream of error corrections to assess. The assessment is simulated by comparing the VI before and after each performed correction. Corrections are accepted only when VI reduces, and we test this across different user error rates (Fig.~\ref{fig:results}). In Haehn et al., the proofreading time was limited to 30 minutes, and human participants performed 59 corrections on average ($\approx30$ seconds per correction). In our scenario, users do not need to visually find errors and manually correct them, and so instead we assume each correction assessment takes 15 seconds (120 assessments in 30 minutes). Split errors are likely to take 

\begin{wrapfigure}{r}{0.5\textwidth}
  \vspace{-0.95cm}
  \begin{center}
    \includegraphics[scale=.1]{gfx/new_er.pdf}
  \end{center}
  \vspace{-1cm}
  \caption{VI vs.~sim.~user error rate.}
  \vspace{-1.5cm}
\end{wrapfigure}

\noindent less time than this; however, merge errors are harder to assess, as the user must select between the top 5 candidate boundaries. Since the performance between human participants of Haehn et al.'s user study shows large variation, we present both the best performing user (VI improvement: $0.0102$) and the average performance among all users (VI improvement: $-0.0582$) as our baseline. For our simulated user, the VI improvement is $0.0502$ (Fig.~\ref{fig:results}).

%, and we test this across different user error rates





\begin{figure}[t]
 \centering
    \subfloat[Split error\label{fig:layers}]{%
      \includegraphics[width=0.47\textwidth]{gfx/proto_split.png}
    }
    \hfill
    \subfloat[Merge error\label{fig:networks}]{%
      \includegraphics[width=0.49\textwidth]{gfx/proto_merge.png}
    }
	\caption{Our web-based user interface includes a slice overview with the relevant area highlighted in yellow. The interface shows (a) a split error with a suggested correction as well as (b) a merge error with correction. The user selects whether to accept a correction or to skip it.}
	\vspace{-0.4cm}
\end{figure}

\paragraph{Random recommendations.} We decided to test a classifier with random performance in comparison to our learned CNN. For split errors, the simulated user is presented with randomly picked boundaries, which they can accept or reject. For merge errors, the simulated user is presented with 5 randomly selected boundaries from the interior of the segmented region. The significantly worse performance of this approach demonstrates that our network is informative to the user.

\paragraph{Automatic correction.} As a comparison, we also perform automatic correction. During training, we define a probability threshold $p_t=0.95$ for automatic split correction based on CNN probability from the test set. Then, for automatic correction, we apply both classifiers to produce lists of split and merge errors sorted by confidence. First, we correct merge errors with $\max(1-p)$, followed by split error correction using $p_t$. The total time for correcting all errors was 17 minutes on a 3.2 GHz Quad-core Intel Xeon with an NVIDIA GeForce Titan (merge error correction 15min, split error correction 2min). The median VI improvement in comparison to the ground truth was negative, at $-0.0552$ (Fig.~\ref{fig:results}). This is not surprising, as the problem is very challenging, and this motivates the need for human-in-the-loop proofreading tools.

\begin{figure}[ht]
 \centering
    \subfloat[Split error\label{fig:layers}]{%
      \includegraphics[width=0.49\textwidth]{gfx/cylinder_vi.pdf}
    }
    \hfill
    \subfloat[Merge error\label{fig:networks}]{%
      \includegraphics[width=0.49\textwidth]{gfx/simuser_vi.pdf}
    }
	\caption{Our web-based user interface includes a slice overview with the relevant area highlighted in yellow. The interface shows (a) a split error with a suggested correction as well as (b) a merge error with correction. The user selects whether to accept a correction or to skip it.}
	\vspace{-0.4cm}
\end{figure}

%
%
%\subsection{Split error evaluation}
%
%Paragraph: What is the process of evaluating split errors?
%
%Paragraph: What do we compare against? What is the result? Why is the performance better?
%
%\begin{table}[t]
%\begin{tabular}{ll}
%\toprule
%Method & VI improvement after fixing split errors \\
%\midrule
%Jain design & \\
%Jain design variation & \\
%Our design &  \\
%Our design variation & \\
%\bottomrule
%\end{tabular}
%\caption{This is a table of results. It shows the comparison to Jain et al., and the comparison to different variations of these algorithms with the varying overlap regions.}
%\label{tab:spliterrorcorrectionperformance}
%\end{table}
%
%\subsubsection{Analysis}
%
%Paragraph: Demonstration of ROC curves for VI performance in split error adjustment as the threshold varies.
%
%\begin{figure}[t]
%\missingfigure{}
%\caption{What does the performance of split error correction look like (ROC curve) as the threshold on edge probability changes?}
%\end{figure}
%
%\subsection{Merge error evaluation}
%
%Merge errors are not that common. False positive rate is very important. Choosing threshold is important.
%
%Paragraph: What is the process of evaluating merge errors?
%
%Paragraph: What do we compare against? What is the result? Why is the performance better?
%
%\begin{table}[t]
%\begin{tabular}{ll}
%\toprule
%Method & VI improvement after fixing merge errors \\
%\midrule
%Our design &  \\
%Our design variation & \\
%\bottomrule
%\end{tabular}
%\caption{This is a table of results. It shows our ability to improve VI.}
%\end{table}
%
%
%
%Philosophical point of trading split errors for merge errors...
%
%
%Speed of classification