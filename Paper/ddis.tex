% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
\documentclass{llncs}
%
\usepackage{makeidx}  % allows for indexgeneration
\usepackage{todonotes}
%

\begin{document}
%
\title{Semi-supervised learning in quality metrics for Electron Microscope data}
%
\titlerunning{Quality Metrics for Electron Microscope data}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Anon}
%
\authorrunning{Anon et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Anon}
%
\institute{Anon\\
\email{anon@anon.edu},\\ WWW home page:
\texttt{http://anon.com/}
}

\maketitle              % typeset the title of the contribution

\begin{abstract}
%Modern electron microscopes acquire image sections at rates of multiple gigabytes per second. Often storing and distributing the dataset is difficult. Rendering requires a registered, stitched, and pyramid decomposed copy of the data, which can take days to compute. Increases in microscope acquisition rates outpace decreases in the time to compute and distribute the data for viewing, and so a different approach is needed. We propose a demand-driven image server for volume viewing which moves towards processing only the requested user display pixels on the fly from the unprocessed dataset. To remove dataset re-storage and re-distribution, clients access the data from the web: the browser viewport details are sent to the server, which replies with just the required pixels. This enables immediate presentation and exploration after data acquisition. We demonstrate the performance of our image server on two large Connectome datasets. We evaluate our infrastructure on a simulated infinitely large volume to validate scalability. We discuss our findings and provide an analysis of requirements for demand-driven rendering of large image volumes.
Modern electron microscopes can acquire image volumes at very high rate with nano scale resolution. Physical constraints of the machines as well as variation in sample quality result in very variable image quality in-plane and between image sections. Scientists assess the acquired image data qualitatively by using simple quality measures. They rescan data which appears to not be useful. Nevertheless, qualitatively poor data might be quantitatively sufficient for analysis and a high percentage of rescans could be avoided. In addition, manual assessment of images is extremely time consuming and not feasible for the large amount of generated data. This results in a reduced throughput of the electron microscope and ultimately creates a gap between theoretical and practical acquisition speed.

We studied the image quality assessment process performed by scientists after acquisition of electron microscope data. Our research shows that even though scientists perceive data as not useful, XX percent of this data would be sufficient for analysis. Therefor, we propose a novel objective image quality metric which takes post-processing such as contrast normalization, deblurring and segmentation into account. Our metric is based on a classifier trained using semi-supervised learning and delivers a stable assessment of acquired data for analysis. We implemented our metric as part of a demand driven visualization framework for a modern multibeam electron microscope.

We evaluate our metric on multiple Connectome datasets of different quality. Our results show an overall decrease in required rescanning attempts.

\end{abstract}
\keywords{Demand-driven rendering, electron microscopy, image server.}

%
\section{Introduction}
- modern electron microscopes produce a ton of data\\
- scientists judge the quality of the data using their experience or simple quality metrics\\
- we think that a lot of data has sufficient quality for automatic analysis even though the scientists' perception thinks it is bad data\\
- a lot of research has been performed on image quality metrics to simulate the human visual system\\
- subjective metrics\\
- objective metrics\\
- we think that in Connectomics, the human visual system is not the right classifier for deciding if images can be used or not\\
- therefor, previous research does not yield good results\\

we show variable quality

we show the current approach of quality assessment and segmentation results for good and bad cases based on it

we describe our metric

we evaluate our metric





%
%
\subsection{Related Work}
%Won-ki work on demand-driven volumetric data
%
%Won-ki work on demand-driven image editing for gigapixel imaging - pyramids still kept on disk, and platform is a desktop software application.
%
%\ref{http://journal.frontiersin.org/article/10.3389/fninf.2014.00089/full?utm_source=newsletter&utm_medium=email&utm_campaign=Neuroscience-w9-2015}
%
%Cite Stackdrop and XTK and Dojo for web-based rendering of medical data.
%
%However, none of these systems account for storage and distribution issues in their approaches. The consideration of storage, distribution, and viewing at the same time must form our approach.
%%
see other document



\section{Image Quality of EM data}

\subsection{Acquisition process of a multibeam microscope}

Briefly describe how the acquisition works (sections on wafer, focus sampling, different mFoVs etc.)

\subsection{Variable Image Quality}

Show subjective image quality discrepancies a) in-plane and b) per image section which result from mechanical constraints and other reasons

We should probably have some quantitative measures for this based on several datasets.

\begin{figure}
\missingfigure{In-plane image quality discrepancies and per section image quality discrepancies}
\caption{\emph{Left:} In-plane. \emph{Right:} Per section.}
\end{figure}

\subsection{Traditional Quality Assessment}

Talk about how scientists measure image quality using their experience and also simple metrics.

Maybe describe Josh's metric which is implemented in Matlab.

Show examples of good data and bad data and the results of the segmentation of both. Also, maybe show Josh's metric result. Maybe compare VI against a small manually labeled sub-set?

\begin{figure}
\missingfigure{Good images and bad images and their segmentation result}
\caption{\emph{Left:} Good image and the segmentation/analysis result. \emph{Right:} Bad image and the segmentation/analysis result.}
\end{figure}

%
%Server cluster with one PC per tile :  )
%
%\begin{enumerate}
%\item Websocket canvas viewer
%	\begin{enumerate}
%	\item add basic interaction
%	\item add simple 3d rendering of current viewport $+-5$ in z using WebGL
%	\end{enumerate}
%\item Adjust openseadragon interface without advanced caching (this is one of the limitations)
%\item Adjust slide atlas interface without advanced caching (another limitation due to Ajax)
%\item Finish prefetching
%\item Add blending while stitching as in 
%\item Adi: local MLS implementation in OpenCL?
%\end{enumerate}

\section{Our Metric}

\subsection{Preprocessing}
- contrast normalization based on lookup table from microscope\\
- deblurring using dark channel prior (Jinshan's work)\\
- segmentation using rhoana or something faster

\subsection{Feature Collection}
- compare VI in-plane and across sections (but against what?)
- maybe use segmentation quality assessment?

\subsection{Learning}
- learn from each classification but how?

\subsection{Classification}

\subsection{The System}
- the mbeam viewer / butterfly server

\section{Evaluation and Results}

- we test our metric against the traditional quality assessment process
- and maybe against other previously published methods???\\
- we hopefully find that a lot of data does not need to be rescanned when using our processing and hopefully the metric shows that


\begin{figure}
\missingfigure{Examples}
\caption{\emph{Left:} System in use with a cool dataset. \emph{Right:} System in use with another cool dataset. Wow.}
\end{figure}
%
%Examples: What datasets can we use? Alyssa, Josh
%
%Evaluation: using the 1 or 2 datasets (see above) and maybe a simulated “infinite” datastream. What was the intended evaluation here? Performance?
%
%The slide atlas and openseadragon performance vs. our websocket

\begin{table}
\missingfigure{Examples}
\caption{Quantitative performance results. So good.}
\end{table}

\begin{figure}
\missingfigure{Plot of microscope throughput / rescanning}
\caption{\emph{Shows hopefully that a zickzack curve gets smoothed out}}
\end{figure}


\section{Discussion}

Limitations of current approach

%Look at current bottlenecks in pipeline; project out bottlenecks, propose solutions to those bottlenecks.


%
% ---- Bibliography ----
%
\begin{thebibliography}{}

\end{thebibliography}

\end{document}
