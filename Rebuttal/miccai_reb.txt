We thank the reviewers for their constructive comments.
We will fix all minor issues.

There were two major issues:
1) The size and singularity of our dataset
2) The lack of a real user

Since our initial submission, we have corrected both issues and recomputed our results.

1) We have added a new dataset (blue 3-cylinder volume - Kasthuri et al. Cell 162, 2015). This increases the training data for our CNN to 2048 x 2048 x 250 voxels (vx) with 266088 correct regions and 266088 split error patches (previously 1024 x 1024 x 70 vx, 79828 correct and 79828 error patches). This yields better results on our (now also larger) test data (2048 x 2048 x 50 vx, previously 1024 x 1024 x 5 vx):

- New data: train. loss 0.045, test loss 0.064, precision/recall 0.90, f1-score 0.90, test acc. 90.12%
- Existing: train. loss 0.370, test loss 0.390, precision/recall 0.83, f1-score 0.83, test acc. 83.02%


2) We have performed a user experiment with 1 novice and 2 expert users, where our system recommended errors with suggested corrections on an interactive website. Each user tested both our existing network and our new re-trained network (3 users x 2 trials; avg. num. error decisions = ~450 per trial in 30 minutes). 

For our simulated user, we budgeted 15 seconds for each split/merge error decision. However, from real-world user performance, the avg. time was ~3.2 seconds. Hence, our new budget is 5 seconds.



Updated results (test on existing data):
 
All results are reported as median Variation of Information (the lower the better):
Novice 0.443 (new network), 0.424 (old network)
Expert 1 0.415 (new network), 0.43 (old network)
Expert 2 0.396 (new network), 0.407 (old network)
Our simulated user 0.394 (new network), 0.402 (old network, new timing), initially 0.426
Random Recommendations 0.477 (new network), 0.472 (old network, new timing), initially 0.475
Automatic Corrections 0.498 (new network), 0.536 (old network)

New median VI scores:

- R2: Total # of errors: how many splits / merges?

New results:

To strengthen our evaluation, we also tested against the new larger dataset (blue 3-cylinder volume):

Automatic Segmentation: 0.485
Automatic Corrections: 0.434 (new network), 0.479 (old network)



Specific reviewer comments:

- R1: VI unintuitive as segmentation quality measure: True; we will include examples in supplementary material to help demonstrate.

- R1: Noisy results: Our new experiments on larger data present clearer performance indications, as you suggest.
 
- R1: Only 2D: 3D information _would_ help the network to classify, though typically this requires expensive slice alignment to obtain volumetric data. In 2D, segmentation proofreading can begin immediately without alignment which is crucial in our processing pipeline.

- R1: Random recommendations: all non errors?
 
- R1: Counter-intuitive user results (Dojo experiment):
Perhaps it is more understandable given that Haehn et al. chose completely unexperienced participants in their Dojo study to simulate an "extreme case". We took this to imply a 'crowdsourcing' scenario where potentially thousands of people tackle large proofreading tasks. In this case, our improved approach may hold merit.

- R2: Patch size: 75x75, defined to cover ~80% of all boundaries in our segmentation output.



For your convenience, we have added updated plots representing the new results in this rebuttal to an anonymous website, along with a small video demonstrating the user study interface as well as example corrections: http://anonmiccai2016.github.io/


Thanks!
